{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Piccolo\n",
    "\n",
    "using LinearAlgebra\n",
    "using Optim\n",
    "using Random\n",
    "\n",
    "using CairoMakie\n",
    "using ForwardDiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example I.1\n",
    "-----\n",
    "**How to evaluate Newton's method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function h(x)\n",
    "    return x.^4 + x.^3 - x.^2 - x\n",
    "end\n",
    "\n",
    "function ‚àáh(x)\n",
    "    return 4.0*x.^3 + 3.0*x.^2 - 2.0*x - 1.0\n",
    "end\n",
    "\n",
    "function ‚àá¬≤h(x)\n",
    "    return 12.0*x.^2 + 6.0*x - 2.0\n",
    "end\n",
    "\n",
    "x = range(-1.75,1.25,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function newton_step(x·µ¢)\n",
    "    return x·µ¢ - ‚àá¬≤h(x·µ¢)\\‚àáh(x·µ¢)\n",
    "end\n",
    "\n",
    "## Initial guess\n",
    "x·µ¢ = 1.19\n",
    "# x·µ¢ = 0.0\n",
    "\n",
    "## Initial plot\n",
    "fig1 = Figure()\n",
    "ax1 = Axis(fig1[1,1])\n",
    "lines!(ax1, x, h(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x·µ¢‚Çä‚ÇÅ = newton_step(x·µ¢) \n",
    "plot!(ax1, [x·µ¢], [h(x·µ¢)], color=:orange, marker='x', markersize=25)\n",
    "x·µ¢ = x·µ¢‚Çä‚ÇÅ\n",
    "fig1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function regularized_newton_step(x·µ¢)\n",
    "    Œ≤ = 1.0\n",
    "    H = ‚àá¬≤h(x·µ¢)\n",
    "    while !isposdef(H)\n",
    "        H = H + Œ≤*I\n",
    "    end\n",
    "    return x·µ¢ - H\\‚àáh(x·µ¢)\n",
    "end\n",
    "\n",
    "## Initial guess\n",
    "# x·µ¢ = 1.19\n",
    "x·µ¢ = 0.0\n",
    "\n",
    "## Initial plot\n",
    "fig1 = Figure()\n",
    "ax1 = Axis(fig1[1,1])\n",
    "lines!(ax1, x, h(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x·µ¢‚Çä‚ÇÅ = regularized_newton_step(x·µ¢) \n",
    "plot!(ax1, [x·µ¢], [h(x·µ¢)], color=:red, marker='x', markersize=25)\n",
    "x·µ¢ = x·µ¢‚Çä‚ÇÅ\n",
    "fig1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function backtracking_regularized_newton_step(x·µ¢)\n",
    "    H = ‚àá¬≤h(x·µ¢)\n",
    "\n",
    "    ## regularization\n",
    "    Œ≤ = 1.0\n",
    "    while !isposdef(H)\n",
    "        H = H + Œ≤*I\n",
    "    end\n",
    "    Œîx = -H\\‚àáh(x·µ¢)\n",
    "\n",
    "    ## line search\n",
    "    b = 0.1\n",
    "    c = 0.25\n",
    "    Œ± = 1.0\n",
    "    while h(x·µ¢ + Œ±*Œîx) > h(x·µ¢) + b*Œ±*‚àáh(x·µ¢)*Œîx\n",
    "        Œ± = c*Œ±\n",
    "    end\n",
    "    \n",
    "    return x·µ¢ + Œ±*Œîx\n",
    "end\n",
    "\n",
    "## Initial guess\n",
    "# x·µ¢ = 1.19\n",
    "x·µ¢ = 0.0\n",
    "\n",
    "## Initial plot\n",
    "fig1 = Figure()\n",
    "ax1 = Axis(fig1[1,1])\n",
    "lines!(ax1, x, h(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x·µ¢‚Çä‚ÇÅ = backtracking_regularized_newton_step(x·µ¢) \n",
    "plot!(ax1, [x·µ¢], [h(x·µ¢)], color=:green, marker='x', markersize=25)\n",
    "x·µ¢ = x·µ¢‚Çä‚ÇÅ\n",
    "fig1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example I.2\n",
    "-----\n",
    "**How to set up and solve a GRAPE problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GRAPE algorithm comes from NMR in 2004: https://doi.org/10.1016/j.jmr.2004.11.004\n",
    "\n",
    "![GRAPE algorithm](images/gr1.gif)\n",
    "\n",
    "A Julia version has been written: https://github.com/JuliaQuantumControl/GRAPE.jl\n",
    "\n",
    "We'll reproduce GRAPE in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a quantum system using Piccolo. Quantum systems store Hamiltonians. `PAULIS` and `GATES` contain some helpful matrices. We pick a system with a little drift, just so we don't have an obvious solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Arguments are H_drift, [H_controls]\n",
    "system = QuantumSystem(0.01 * PAULIS[:Z], [PAULIS[:X], PAULIS[:Y]])\n",
    "U_goal = GATES[:X]\n",
    "\n",
    "T = 25\n",
    "Œît = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easiest to work with real-valued variables, especially when we want to use tools like automatic differentiation. Piccolo has a number of isomorphism utilities in `QuantumCollocation::Isomorophisms.jl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UÃÉ‚Éó_goal = operator_to_iso_vec(U_goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not too hard to convert back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_vec_to_operator(UÃÉ‚Éó_goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `QuantumCollocation::Rollouts.jl` to get a trajectory from the controls. Notice that we pass a controls matrix and a timestep vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(1234)\n",
    "\n",
    "a_guess = randn(2, T)\n",
    "\n",
    "## Rollout returns the unitary at each time step (as a vector)\n",
    "UÃÉ‚Éós = unitary_rollout(a_guess, fill(Œît, T), system)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the trajectory of œÉ‚Çì\n",
    "fig1 = Figure()\n",
    "ax1_1 = fig1[1, 1] = Axis(fig1, xlabel = \"Time\", ylabel = \"‚ü®X(t)‚ü©\")\n",
    "ax1_2 = fig1[2, 1] = Axis(fig1, xlabel = \"Time\", ylabel = \"‚ü®Y(t)‚ü©\")\n",
    "ax1_3 = fig1[3, 1] = Axis(fig1, xlabel = \"Time\", ylabel = \"‚ü®Z(t)‚ü©\")\n",
    "\n",
    "function plot_paulis(a)\n",
    "    œà_0 = [1; 0]\n",
    "\n",
    "    x = Float64[]\n",
    "    y = Float64[]\n",
    "    z = Float64[]\n",
    "    for UÃÉ‚Éó ‚àà eachcol(unitary_rollout(a, fill(Œît, T), system))\n",
    "        U = Matrix{ComplexF64}(iso_vec_to_operator(UÃÉ‚Éó))\n",
    "        œà_t = U * œà_0\n",
    "        push!(x, real(œà_t'PAULIS[:X]*œà_t))\n",
    "        push!(y, real(œà_t'PAULIS[:Y]*œà_t))\n",
    "        push!(z, real(œà_t'PAULIS[:Z]*œà_t))\n",
    "    end\n",
    "    lines!(ax1_1, Œît * (1:T), x)\n",
    "    lines!(ax1_2, Œît * (1:T), y)\n",
    "    lines!(ax1_3, Œît * (1:T), z)\n",
    "    return fig1\n",
    "end\n",
    "\n",
    "plot_paulis(a_guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlucky. Our random guess did not implement the X gate üòî. \n",
    "\n",
    "We will have to adjust the controls. `ForwardDiff.jl` allows us to compute the gradient of a scalar objective--in this case, infidelity. You can check that the utility we use here is equivalent to the Hilbert Schmidt norm (the usual unitary infidelity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f(x::AbstractVector)\n",
    "    a = reshape(x, (2, T))\n",
    "    UÃÉ‚Éó = unitary_rollout(a, fill(Œît, T), system)\n",
    "    return 1 - iso_vec_unitary_fidelity(UÃÉ‚Éó[:, end], UÃÉ‚Éó_goal)\n",
    "end\n",
    "\n",
    "‚àáf(x) = ForwardDiff.gradient(f, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initial condition\n",
    "x·µ¢ = copy(vec(a_guess));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradient descent\n",
    "\n",
    "# Learning rate\n",
    "Œª = 0.25\n",
    "\n",
    "# Step\n",
    "x·µ¢‚Çä‚ÇÅ = x·µ¢ - Œª * ‚àáf(x·µ¢)\n",
    "\n",
    "f(x·µ¢‚Çä‚ÇÅ) |> println\n",
    "x·µ¢ .= x·µ¢‚Çä‚ÇÅ\n",
    "plot_paulis(reshape(x·µ¢‚Çä‚ÇÅ, (2, T)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The controls seem to do the job! Let's compare the original and the optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function plot_control(a, title=\"\")\n",
    "    fig = Figure()\n",
    "\n",
    "    ax1 = Axis(fig[1, 1], title=\"Initial\", xlabel = \"Time\", ylabel = \"Control\")\n",
    "    stairs!(ax1, Œît * (1:T), a_guess[1, :])\n",
    "    stairs!(ax1, Œît * (1:T), a_guess[2, :])\n",
    "\n",
    "    ax2 = fig[1, 2] = Axis(fig, title=title, xlabel = \"Time\")\n",
    "    stairs!(ax2, Œît * (1:T), a[1, :])\n",
    "    stairs!(ax2, Œît * (1:T), a[2, :])\n",
    "    return fig\n",
    "end\n",
    "\n",
    "## Plot the optimized control\n",
    "a_optimized = reshape(x·µ¢‚Çä‚ÇÅ, (2, T))\n",
    "plot_control(a_optimized, \"Gradient descent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we don't need to code up our own gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = optimize(f, ‚àáf, vec(a_guess), GradientDescent(), inplace=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the optimized control\n",
    "a_optimized = reshape(res.minimizer, (2, T))\n",
    "plot_control(a_optimized, \"GradientDescent (Optim.jl)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise I.1\n",
    "**Accelerate GRAPE using Newton's method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Isaac Newton driving a car.](images/newton.png)\n",
    "\n",
    "Let's go faster. In this exercise, we will implement various versions of second-order GRAPE: https://doi.org/10.1016/j.jmr.2011.07.023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the Hessian from f\n",
    "‚àá¬≤f(x) = ForwardDiff.hessian(f, x)\n",
    "\n",
    "function Œîx(x)\n",
    "   return -‚àá¬≤f(x) \\ ‚àáf(x)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the Newton step\n",
    "Œîx(vec(a_guess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous cell should remind us about the importance of regularization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f_reg(x; Œª=1e-8)\n",
    "    f(x) + Œª * x'x\n",
    "end\n",
    "\n",
    "function ‚àáf_reg!(G, x)\n",
    "    ForwardDiff.gradient!(G, f_reg, x)\n",
    "end\n",
    "\n",
    "function ‚àá¬≤f_reg!(H, x)\n",
    "    ForwardDiff.hessian!(H, f_reg, x)\n",
    "end\n",
    "\n",
    "function Œîx_reg(x)\n",
    "    G = zeros(length(x))\n",
    "    H = zeros(length(x), length(x))\n",
    "    ‚àá¬≤f_reg!(H, x)\n",
    "    ‚àáf_reg!(G, x)\n",
    "    return -H \\ G\n",
    " end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Œîx_reg(vec(a_guess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should have gone better. Now, let's try to solve our regularized problem three ways using optim:\n",
    "\n",
    "Gradient and Hessian:\n",
    "\n",
    "1. Newton's method (Optim implements a line search for us!)\n",
    "2. Newton's method with a trust region\n",
    "\n",
    "Gradient only:\n",
    "\n",
    "3. L-BFGS\n",
    "\n",
    "The **Optim.jl** documentation will help: https://julianlsolvers.github.io/Optim.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pass these options to optimize to limit the iterations and save function evaluations.\n",
    "optim_options = Optim.Options(iterations=10, store_trace=true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Newton's method\n",
    "res_newton = optimize(\n",
    "    f_reg, ‚àáf_reg!, ‚àá¬≤f_reg!, vec(a_guess), Newton(),\n",
    "    optim_options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Newton's method with a trust region (dual to line search)\n",
    "res_newton_tr = optimize(\n",
    "    f_reg, ‚àáf_reg!, ‚àá¬≤f_reg!, vec(a_guess), NewtonTrustRegion(),\n",
    "    optim_options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LBFGS\n",
    "res_lbfgs = optimize(\n",
    "    f_reg, ‚àáf_reg!, vec(a_guess), LBFGS(),\n",
    "    optim_options\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the convergence rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3 = Figure()\n",
    "ax3 = fig3[1, 1] = Axis(fig3, xlabel=\"Iteration\", ylabel=\"Objective\", yscale=log10)\n",
    "\n",
    "## Add a gradient descent for comparison\n",
    "res_gd = optimize(f_reg, ‚àáf_reg!, vec(a_guess), GradientDescent(), optim_options)\n",
    "\n",
    "lines!(ax3, Optim.f_trace(res_newton), label=\"Newton\")\n",
    "lines!(ax3, Optim.f_trace(res_newton_tr), label=\"NewtonTrustRegion\")\n",
    "lines!(ax3, Optim.f_trace(res_lbfgs), label=\"LBFGS\")\n",
    "lines!(ax3, Optim.f_trace(res_gd), label=\"GradientDescent\")\n",
    "\n",
    "Legend(fig3[1, 2], ax3, nbanks=1)\n",
    "fig3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everyone drives a lot faster than Newton's method for our high dimensional problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise I.2\n",
    "**Add a function basis to GRAPE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/crab.jpg\" alt=\"CRAB algorithm\" style=\"width:800px;\"/>\n",
    "\n",
    "The previous control solutions seemed pretty jumpy and wild. Can we do better? \n",
    "- One decade of CRAB: https://doi.org/10.1088/1361-6633/ac723c\n",
    "\n",
    "What are some other good functions to use?\n",
    "- Slepians provide bandwidth limits: https://doi.org/10.1103/PhysRevA.97.062346\n",
    "- Splines minimize the curvature between points: https://github.com/LLNL/Juqbox.jl\n",
    "\n",
    "The image is showing the idea of a _control landscape_. There are two versions, the landscape over the control parameters $c_1$ and $c_2$ (left) and the landscape over the final state (right). The point is that, while fidelity is a convex function with one maximum, control landscapes can be much more complicated.\n",
    "\n",
    "In this example, we will pick a set of basis functions and expand the control in that basis,\n",
    "\\begin{equation}\n",
    "    a(t) = b_0 + \\sum_{j=1}^n \\theta_j b_j(t).\n",
    "\\end{equation}\n",
    "The optimizable parameters are now the coefficients in this basis. The idea here is _dimensionality reduction_ to simplify the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First n=5 entries in a Fourier series, including the constant term\n",
    "n = 5\n",
    "fourier_series = [cos.(œÄ * j * (0:T-1) / T .- œÄ/2) for j in 0:n-1]\n",
    "\n",
    "function expand_in_basis(Œ∏::AbstractVector; basis=fourier_series)\n",
    "    ## Convert the coefficients to a control vector\n",
    "    return sum(Œ∏·µ¢ * b·µ¢ for (Œ∏·µ¢, b·µ¢) in zip(Œ∏, basis))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function g(Œ∏)\n",
    "    a = [expand_in_basis(Œ∏[1:end √∑ 2]); expand_in_basis(Œ∏[end √∑ 2 + 1:end])]\n",
    "    return f(a)\n",
    "end\n",
    "\n",
    "‚àág(Œ∏) = ForwardDiff.gradient(g, Œ∏)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimize using LBFGS\n",
    "Random.seed!(1234)\n",
    "\n",
    "res_fourier = optimize(g, ‚àág, rand(2n), LBFGS(), inplace=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the optimized control\n",
    "a_optimized = [\n",
    "    reshape(expand_in_basis(res_fourier.minimizer[1:end √∑ 2]), (1,T));\n",
    "    reshape(expand_in_basis(res_fourier.minimizer[end √∑ 2:end]), (1,T))\n",
    "]\n",
    "\n",
    "plot_control(a_optimized, \"Fourier series\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
