{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Piccolo\n",
    "\n",
    "using LinearAlgebra\n",
    "using Random\n",
    "\n",
    "using CairoMakie\n",
    "using ForwardDiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example II.1\n",
    "-----\n",
    "**How the KKT system works**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time, we learned about gradient descent and Newton's method. We also learned that, without guarantees on convexity, we needed techniques like regularization and line search to actually make sure our optimization routines worked.\n",
    "\n",
    "In this section, we are going to learn about *constrained optimization*. Let's keep things simple, and try to avoid all of the regularization and line search stuff. In the next cells, we'll define a 2D bowl as our cost function, and we'll draw some nice level curves to visualize it--it's a convex cost, so we know it will have a minimum at the bottom of the bowl. To make it interesting, we will add a single constraint, which we draw as a level curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = Diagonal([0.5; 1])\n",
    "\n",
    "## Objective\n",
    "function J(x)\n",
    "    return 1 / 2 * (x - [1; 0])' * Q * (x - [1; 0])\n",
    "end\n",
    "\n",
    "function ∇J(x)\n",
    "    return Q * (x - [1; 0])\n",
    "end\n",
    "\n",
    "function ∇²J(x)\n",
    "    return Q\n",
    "end\n",
    "\n",
    "## Linear constraint -- you can try this, also.\n",
    "# A = [1.0 -1.0]\n",
    "# b = -1.0\n",
    "# function f(x)\n",
    "#     return A * x - b\n",
    "# end\n",
    "\n",
    "# function ∂f(x)\n",
    "#     return A\n",
    "# end\n",
    "\n",
    "## Nonlinear constraint\n",
    "function f(x)\n",
    "    return x[1]^2 + 2*x[1] - x[2]\n",
    "end\n",
    "\n",
    "function ∂f(x)\n",
    "    return [2*x[1]+2 -1]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function draw_contour(ax; samples=40, levels=25)\n",
    "    cols = kron(ones(samples), range(-4, 4, samples)')\n",
    "    rows = kron(ones(samples)', range(-4, 4, samples))\n",
    "    vals = zeros(samples,samples)\n",
    "    for j = 1:samples\n",
    "        for k = 1:samples\n",
    "            vals[j, k] = J([cols[j, k]; rows[j, k]])\n",
    "        end\n",
    "    end\n",
    "    contour!(ax, vec(cols), vec(rows), vec(vals), levels=levels)\n",
    "\n",
    "    ## Linear x - y + 1 = 0 -- uncomment this if you want to try linear constraint\n",
    "    # constraint = range(-4, 3, samples)\n",
    "    # lines!(ax, constraint, constraint .+ 1, color=:black, linewidth=2)\n",
    "\n",
    "    ## Nonlinear x^2 + 2x - y = 0\n",
    "    constraint = range(-3.2, 1.2, samples)\n",
    "    lines!(ax, constraint, constraint.^2 .+ 2*constraint, color=:black, linewidth=2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function newton_step(xᵢ, λᵢ)\n",
    "    ∂²L_∂x² = ∇²J(xᵢ) + ForwardDiff.jacobian(x -> ∂f(x)'λᵢ, xᵢ)\n",
    "    ∂f_∂x = ∂f(xᵢ)\n",
    "\n",
    "    ## KKT system\n",
    "    H = [∂²L_∂x² ∂f_∂x'; ∂f_∂x 0]\n",
    "    g = [∇J(xᵢ) + ∂f_∂x'λᵢ; f(xᵢ)]\n",
    "    \n",
    "    Δz = -H\\g\n",
    "    Δx = Δz[1:2]\n",
    "    Δλ = Δz[3]\n",
    "    return xᵢ .+ Δx, λᵢ .+ Δλ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = Figure()\n",
    "ax = Axis(fig[1,1], aspect=1)\n",
    "\n",
    "## Initial guess\n",
    "# xᵢ = Float64[-0.75; -1.75]\n",
    "xᵢ = Float64[-3; 2]\n",
    "λᵢ = Float64[0.0]\n",
    "\n",
    "## Draw the initial contours and the initial guess\n",
    "draw_contour(ax)\n",
    "plot!(ax, [xᵢ[1]], [xᵢ[2]], color=:red, marker=:circle, markersize=15)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xᵢ₊₁, λᵢ₊₁ = newton_step(xᵢ, λᵢ)\n",
    "plot!(ax, [xᵢ₊₁[1]], [xᵢ₊₁[2]], color=:red, marker=:x, markersize=15)\n",
    "xᵢ .= xᵢ₊₁\n",
    "λᵢ .= λᵢ₊₁\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspect the Hessian\n",
    "H = ∇²J(xᵢ) + ForwardDiff.jacobian(x -> ∂f(x)'λᵢ, xᵢ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need regularization... even though we picked a convex cost! The constraint in our system messed things up. Let's add regularization, but we will do so a bit differently.\n",
    "\n",
    "Recall from Part I, where we learned about LBFGS approximation to Newton's method, and how the LBFGS seemed to have robustness properites that Newton's method lacked. We will make a similar approximation to our KKT system, called the *Gauss-Newton approximation*. See the exercises below for a more detailed explanation.\n",
    "\n",
    "The thought process is as follows: After inspecting `∂²L_∂x² = ∇²J(xᵢ) + ForwardDiff.jacobian(x -> ∂f(x)'λᵢ, xᵢ)`, we note that $\\nabla^2 J$ is convex by construction. It is just the `ForwardDiff.jacobian(x -> ∂f(x)'λᵢ, xᵢ)` that causes trouble with the Hessian. At this time, we also notice that latter term is also expensive to compute. Because it causes trouble and is costly to compute, we decide to drop this term. This is the Gauss-Newton approximation. Its steps compute faster, but converge slower than Newton--luckily, the savings in compute speed often overtake any reduction in convergence rate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gauss_newton_step(xᵢ, λᵢ)\n",
    "    ## Implicit regularization\n",
    "    ∂²L_∂x² = ∇²J(xᵢ) #+ ForwardDiff.jacobian(x -> ∂f(x)'λᵢ, xᵢ)\n",
    "    ∂f_∂x = ∂f(xᵢ)\n",
    "\n",
    "    ## KKT system\n",
    "    H = [∂²L_∂x² ∂f_∂x'; ∂f_∂x 0]\n",
    "    g = [∇J(xᵢ) + ∂f_∂x'λᵢ; f(xᵢ)]\n",
    "    \n",
    "    Δz = -H\\g\n",
    "    Δx = Δz[1:2]\n",
    "    Δλ = Δz[3]\n",
    "    return xᵢ .+ Δx, λᵢ .+ Δλ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = Figure()\n",
    "ax = Axis(fig[1,1], aspect=1)\n",
    "\n",
    "## Initial guess\n",
    "# xᵢ = Float64[-0.75; -1.75]\n",
    "xᵢ = Float64[-3; 2]\n",
    "λᵢ = Float64[0.0]\n",
    "\n",
    "draw_contour(ax)\n",
    "plot!(ax, [xᵢ[1]], [xᵢ[2]], color=:green, marker=:circle, markersize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xᵢ₊₁, λᵢ₊₁ = gauss_newton_step(xᵢ, λᵢ)\n",
    "plot!(ax, [xᵢ₊₁[1]], [xᵢ₊₁[2]], color=:green, marker=:x, markersize=15)\n",
    "xᵢ .= xᵢ₊₁\n",
    "λᵢ .= λᵢ₊₁\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example II.2\n",
    "-----\n",
    "**How to interpret Piccolo outputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = QuantumSystem(0.01 * PAULIS[:Z], [PAULIS[:X], PAULIS[:Y]])\n",
    "U_goal = GATES[:X]\n",
    "\n",
    "T = 50\n",
    "Δt = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(1234)\n",
    "ipopt_options = IpoptOptions(max_iter=50, recalc_y=\"yes\", recalc_y_feas_tol=5e1)\n",
    "problem = UnitarySmoothPulseProblem(system, U_goal, T, Δt, ipopt_options=ipopt_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for *inf_pr*, *inf_du*, and *complementarity*! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solve!(problem, max_iter=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the IPOPT documentation (https://coin-or.github.io/Ipopt/OUTPUT.html):\n",
    "\n",
    "- **inf_pr**: The unscaled constraint violation at the current point. This quantity is the infinity-norm (max) of the (unscaled) constraints ( gL≤g(x)≤gU in (NLP)). During the restoration phase, this value remains the constraint violation of the original problem at the current point. The option inf_pr_output can be used to switch to the printing of a different quantity.\n",
    "\n",
    "- **inf_du**: The scaled dual infeasibility at the current point. This quantity measure the infinity-norm (max) of the internal dual infeasibility, Eq. (4a) in the implementation paper [12], including inequality constraints reformulated using slack variables and problem scaling. During the restoration phase, this is the value of the dual infeasibility for the restoration phase problem.\n",
    "\n",
    "- **lg(mu)**: log10 of the value of the barrier parameter μ.\n",
    "\n",
    "- **||d||**: The infinity norm (max) of the primal step (for the original variables x and the internal slack variables s). During the restoration phase, this value includes the values of additional variables, p and n (see Eq. (30) in [12]).\n",
    "\n",
    "- **lg(rg)**: log10 of the value of the regularization term for the Hessian of the Lagrangian in the augmented system ( δw in Eq. (26) and Section 3.1 in [12]). A dash (\"-\") indicates that no regularization was done.\n",
    "\n",
    "- **alpha_du**: The stepsize for the dual variables ( αzk in Eq. (14c) in [12])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unitary_fidelity(problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_unitary_populations(problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise II.1\n",
    "**The Gauss-Newton Approximation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This quick calculation should hopefully remind you about the Gauss-Newton approximation.\n",
    "\n",
    "Start with a cost $J(\\mathbf{x})$. The necessary condition for optimality is $\\nabla J(\\mathbf{x}) = 0$. Our journey starts by asking what happens if $J(\\mathbf{x})$ is actually a least squares problem. For example, $J(\\mathbf{x}) := \\frac{1}{2}||\\mathbf{g}(\\mathbf{x})||_2^2$. \n",
    "\n",
    "**Exercise**\n",
    "1. Compute the Newton step for $J$ in terms of $\\mathbf{g}$.\n",
    "2. There should be two terms in the Hessian. Drop the term that includes a derivative larger than a gradient. This is the Gauss-Newton approximation. When is it ok to drop this term?\n",
    "3. Compare this to the `H` we computed in the KKT system, and see how our Gauss-Newton approximation of the system matches what we did here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise II.2\n",
    "\n",
    "**The augmented Lagrangian**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmented Lagrangians offload penalties onto Lagrange multipliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function augmented_lagrangian(x, λ, ρ=1.0)\n",
    "    p = max(0, f(x))\n",
    "    return J(x) + λ*p + ρ/2 * p'p\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise is **UNDER CONSTRUCTION** -- check back soon!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
