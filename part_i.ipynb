{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using QuantumCollocation\n",
    "\n",
    "using LinearAlgebra\n",
    "using Optim\n",
    "using Random\n",
    "\n",
    "using CairoMakie\n",
    "using ForwardDiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1.1\n",
    "-----\n",
    "**How to evaluate Newton's method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function h(x)\n",
    "    return x.^4 + x.^3 - x.^2 - x\n",
    "end\n",
    "\n",
    "function ∇h(x)\n",
    "    return 4.0*x.^3 + 3.0*x.^2 - 2.0*x - 1.0\n",
    "end\n",
    "\n",
    "function ∇²h(x)\n",
    "    return 12.0*x.^2 + 6.0*x - 2.0\n",
    "end\n",
    "\n",
    "x = range(-1.75,1.25,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function newton_step(xᵢ)\n",
    "    return xᵢ - ∇²h(xᵢ)\\∇h(xᵢ)\n",
    "end\n",
    "\n",
    "## Initial guess\n",
    "# xᵢ = 1.19\n",
    "xᵢ = 0.0\n",
    "\n",
    "## Initial plot\n",
    "fig1 = Figure()\n",
    "ax1 = Axis(fig1[1,1])\n",
    "lines!(ax1, x, h(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xᵢ₊₁ = newton_step(xᵢ) \n",
    "plot!(ax1, [xᵢ], [h(xᵢ)], color = :orange, marker = 'x', markersize=25)\n",
    "xᵢ = xᵢ₊₁\n",
    "fig1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function regularized_newton_step(xᵢ)\n",
    "    β = 1.0\n",
    "    H = ∇²h(xᵢ)\n",
    "    while !isposdef(H)\n",
    "        H = H + β*I\n",
    "    end\n",
    "    return xᵢ - H\\∇h(xᵢ)\n",
    "end\n",
    "\n",
    "## Initial guess\n",
    "# xᵢ = 1.19\n",
    "xᵢ = 0.0\n",
    "\n",
    "## Initial plot\n",
    "fig1 = Figure()\n",
    "ax1 = Axis(fig1[1,1])\n",
    "lines!(ax1, x, h(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xᵢ₊₁ = regularized_newton_step(xᵢ) \n",
    "plot!(ax1, [xᵢ], [h(xᵢ)], color = :red, marker = 'x', markersize=25)\n",
    "xᵢ = xᵢ₊₁\n",
    "fig1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function backtracking_regularized_newton_step(xᵢ)\n",
    "    H = ∇²h(xᵢ)\n",
    "\n",
    "    ## regularization\n",
    "    β = 1.0\n",
    "    while !isposdef(H)\n",
    "        H = H + β*I\n",
    "    end\n",
    "    Δx = -H\\∇h(xᵢ)\n",
    "\n",
    "    ## line search\n",
    "    b = 0.1\n",
    "    c = 0.25\n",
    "    α = 1.0\n",
    "    while h(xᵢ + α*Δx) > h(xᵢ) + b*α*∇h(xᵢ)*Δx\n",
    "        α = c*α\n",
    "    end\n",
    "    \n",
    "    return xᵢ + α*Δx\n",
    "end\n",
    "\n",
    "## Initial guess\n",
    "# xᵢ = 1.19\n",
    "xᵢ = 0.0\n",
    "\n",
    "## Initial plot\n",
    "fig1 = Figure()\n",
    "ax1 = Axis(fig1[1,1])\n",
    "lines!(ax1, x, h(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xᵢ₊₁ = backtracking_regularized_newton_step(xᵢ) \n",
    "plot!(ax1, [xᵢ], [h(xᵢ)], color = :green, marker = 'x', markersize=25)\n",
    "xᵢ = xᵢ₊₁\n",
    "fig1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1.2\n",
    "-----\n",
    "**How to set up and solve a GRAPE problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GRAPE algorithm comes from NMR in 2004: https://doi.org/10.1016/j.jmr.2004.11.004\n",
    "\n",
    "![GRAPE algorithm](images/gr1.gif)\n",
    "\n",
    "A Julia version: https://github.com/JuliaQuantumControl/GRAPE.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a quantum system using Piccolo. `PAULIS` and `GATES` contain some helpful matrices. We pick a system with a little drift, just so we don't have an obvious solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = QuantumSystem(0.01 * PAULIS[:Z], [PAULIS[:X], PAULIS[:Y]])\n",
    "U_goal = GATES[:X]\n",
    "\n",
    "T = 25\n",
    "Δt = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easiest to work with real-valued variables, especially when we want to use tools like automatic differentiation. Piccolo has a number of isomorphism utilities in `QuantumCollocation::Isomorophisms.jl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ũ⃗_goal = operator_to_iso_vec(U_goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not too hard to convert back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_vec_to_operator(Ũ⃗_goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `QuantumCollocation::Rollouts.jl` to get a trajectory from the controls. Notice that we pass the controls and timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(1234)\n",
    "\n",
    "a_guess = randn(2, T)\n",
    "\n",
    "## Rollout returns the unitary at each time step (as a vector)\n",
    "Ũ⃗s = unitary_rollout(a_guess, fill(Δt, T), system)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our random guess did not implement the X gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the trajectory of σₓ\n",
    "fig1 = Figure()\n",
    "ax1_1 = fig1[1, 1] = Axis(fig1, xlabel = \"Time\", ylabel = \"⟨X(t)⟩\")\n",
    "ax1_2 = fig1[2, 1] = Axis(fig1, xlabel = \"Time\", ylabel = \"⟨Y(t)⟩\")\n",
    "ax1_3 = fig1[3, 1] = Axis(fig1, xlabel = \"Time\", ylabel = \"⟨Z(t)⟩\")\n",
    "\n",
    "function plot_paulis(a)\n",
    "    ψ_0 = [1; 0]\n",
    "\n",
    "    x = Float64[]\n",
    "    y = Float64[]\n",
    "    z = Float64[]\n",
    "    for Ũ⃗ ∈ eachcol(unitary_rollout(a, fill(Δt, T), system))\n",
    "        U = Matrix{ComplexF64}(iso_vec_to_operator(Ũ⃗))\n",
    "        ψ_t = U * ψ_0\n",
    "        push!(x, real(ψ_t'PAULIS[:X]*ψ_t))\n",
    "        push!(y, real(ψ_t'PAULIS[:Y]*ψ_t))\n",
    "        push!(z, real(ψ_t'PAULIS[:Z]*ψ_t))\n",
    "    end\n",
    "    lines!(ax1_1, Δt * (1:T), x)\n",
    "    lines!(ax1_2, Δt * (1:T), y)\n",
    "    lines!(ax1_3, Δt * (1:T), z)\n",
    "    return fig1\n",
    "end\n",
    "\n",
    "plot_paulis(a_guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ForwardDiff.jl` allows us to compute the gradient of a scalar objective--in this case, infidelity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f(x::AbstractVector)\n",
    "    a = reshape(x, (2, T))\n",
    "    Ũ⃗ = unitary_rollout(a, fill(Δt, T), system)\n",
    "    return 1 - iso_vec_unitary_fidelity(Ũ⃗[:, end], Ũ⃗_goal)\n",
    "end\n",
    "\n",
    "∇f(x) = ForwardDiff.gradient(f, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initial condition\n",
    "xᵢ = copy(vec(a_guess));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradient descent\n",
    "\n",
    "# Learning rate\n",
    "λ = 0.25\n",
    "\n",
    "# Step\n",
    "xᵢ₊₁ = xᵢ - λ * ∇f(xᵢ)\n",
    "\n",
    "f(xᵢ₊₁) |> println\n",
    "xᵢ .= xᵢ₊₁\n",
    "plot_paulis(reshape(xᵢ₊₁, (2, T)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The controls seem to do the job! Let's compare the original and the optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function plot_control(a, title=\"\")\n",
    "    fig = Figure()\n",
    "\n",
    "    ax1 = Axis(fig[1, 1], title=\"Initial\", xlabel = \"Time\", ylabel = \"Control\")\n",
    "    stairs!(ax, Δt * (1:T), a_guess[1, :])\n",
    "    stairs!(ax, Δt * (1:T), a_guess[2, :])\n",
    "\n",
    "    ax2 = fig[1, 2] = Axis(fig2, title=title, xlabel = \"Time\")\n",
    "    stairs!(ax1, Δt * (1:T), a[1, :])\n",
    "    stairs!(ax2, Δt * (1:T), a[2, :])\n",
    "    return fig2\n",
    "end\n",
    "\n",
    "## Plot the optimized control\n",
    "a_optimized = reshape(xᵢ₊₁, (2, T))\n",
    "plot_control(a_optimized, \"Gradient descent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we don't need to code up our own gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = optimize(f, ∇f, vec(a_guess), GradientDescent(), inplace=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the optimized control\n",
    "a_optimized = reshape(res.minimizer, (2, T))\n",
    "plot_control(a_optimized, \"GradientDescent (Optim.jl)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1.1\n",
    "**Accelerate GRAPE using Newton's method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement second-order GRAPE: https://doi.org/10.1016/j.jmr.2011.07.023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the Hessian from f\n",
    "# ∇²f(x) = # TODO\n",
    "∇²f(x) = ForwardDiff.hessian(f, x)\n",
    "\n",
    "function Δx(x)\n",
    "   # TODO: Implement the Newton's step using backslash\n",
    "   return -∇²f(x) \\ ∇f(x)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the Newton step\n",
    "Δx(vec(a_guess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous cell should remind us about the importance of regularization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f_reg(x; λ=1e-8)\n",
    "    # TODO: Regularize on norm(x) here \n",
    "    return f(x) + λ * x'x\n",
    "end\n",
    "\n",
    "function ∇f_reg!(G, x)\n",
    "    # TODO: In-place gradient computation\n",
    "    ForwardDiff.gradient!(G, f_reg, x)\n",
    "end\n",
    "\n",
    "function ∇²f_reg!(H, x)\n",
    "    # TODO: In-place Hessian computation\n",
    "    ForwardDiff.hessian!(H, f_reg, x)\n",
    "end\n",
    "\n",
    "function ∇²f_reg!(H, x)\n",
    "    # TODO: in-place Hessian computation\n",
    "    ForwardDiff.hessian!(H, f_reg, x)\n",
    "end\n",
    "\n",
    "function Δx_reg(x)\n",
    "    G = zeros(length(x))\n",
    "    H = zeros(length(x), length(x))\n",
    "    ∇²f_reg!(H, x)\n",
    "    ∇f_reg!(G, x)\n",
    "    return -H \\ G\n",
    " end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Δx_reg(vec(a_guess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to solve our regularized problem three ways using optim:\n",
    "\n",
    "Gradient and Hessian:\n",
    "\n",
    "1. Newton's method (Optim implements a line search for us!)\n",
    "2. Newton's method with a trust region\n",
    "\n",
    "Gradient only:\n",
    "\n",
    "3. L-BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pass these options to optimize to limit the iterations and save function evaluations.\n",
    "optim_options = Optim.Options(iterations=10, store_trace=true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Newton's method\n",
    "# res_newton = # TODO \n",
    "\n",
    "res_newton = optimize(\n",
    "    f_reg, ∇f_reg!, ∇²f_reg!, vec(a_guess), Newton(),\n",
    "    optim_options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Newton's method with a trust region\n",
    "# res_newton_tr = # TODO \n",
    "\n",
    "res_newton_tr = optimize(\n",
    "    f_reg, ∇f_reg!, ∇²f_reg!, vec(a_guess), NewtonTrustRegion(),\n",
    "    optim_options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LBFGS\n",
    "# res_lbfgs = # TODO\n",
    "\n",
    "res_lbfgs = optimize(\n",
    "    f_reg, ∇f_reg!, vec(a_guess), LBFGS(),\n",
    "    optim_options\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the convergence rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3 = Figure()\n",
    "ax3 = fig3[1, 1] = Axis(fig3, xlabel=\"Iteration\", ylabel=\"Objective\", yscale=log10)\n",
    "\n",
    "## Add a gradient descent for comparison\n",
    "res_gd = optimize(f_reg, ∇f_reg!, vec(a_guess), GradientDescent(), optim_options)\n",
    "\n",
    "lines!(ax3, Optim.f_trace(res_newton), label=\"Newton\")\n",
    "lines!(ax3, Optim.f_trace(res_newton_tr), label=\"NewtonTrustRegion\")\n",
    "lines!(ax3, Optim.f_trace(res_lbfgs), label=\"LBFGS\")\n",
    "lines!(ax3, Optim.f_trace(res_gd), label=\"GradientDescent\")\n",
    "\n",
    "Legend(fig3[1, 2], ax3, nbanks=1)\n",
    "fig3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everyone else does a lot better than Newton's method for our high dimensional problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1.2\n",
    "**Add a function basis to GRAPE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/crab.jpg\" alt=\"CRAB algorithm\" style=\"width:800px;\"/>\n",
    "\n",
    "The previous control solutions seemed pretty wild. Can we do better? \n",
    "- One decade of CRAB: https://doi.org/10.1088/1361-6633/ac723c\n",
    "\n",
    "What are some other good functions to use?\n",
    "- Slepians provide bandwidth limits: https://doi.org/10.1103/PhysRevA.97.062346\n",
    "- Splines minimize the curvature between points: https://github.com/LLNL/Juqbox.jl\n",
    "\n",
    "The image is showing the idea of a _control landscape_. There are two versions, the landscape over the control parameters $c_1$ and $c_2$ (left) and the landscape over the final state (right). The point is that, while fidelity is a convex function with one maximum, control landscapes can be much more complicated.\n",
    "\n",
    "In this example, we will pick a set of basis functions and expand the control in that basis,\n",
    "\\begin{equation}\n",
    "    a(t) = b_0 + \\sum_{j=1}^n \\theta_j b_j(t).\n",
    "\\end{equation}\n",
    "The optimizable parameters are now the coefficients in this basis. The idea here is _dimensionality reduction_ to simplify the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First n=5 entries in a Fourier series, including the constant term\n",
    "n = 5\n",
    "# fourier_series = # TODO \n",
    "fourier_series = [cos.(π * j * (0:T-1) / T .- π/2) for j in 0:n-1]\n",
    "\n",
    "function expand_in_basis(θ::AbstractVector; basis=fourier_series)\n",
    "    ## Convert the coefficients to a control vector\n",
    "    # return # TODO\n",
    "    return sum(θᵢ * bᵢ for (θᵢ, bᵢ) in zip(θ, basis))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function g(θ)\n",
    "    a = [expand_in_basis(θ[1:end ÷ 2]); expand_in_basis(θ[end ÷ 2:end])]\n",
    "    return f(a)\n",
    "end\n",
    "\n",
    "∇g(θ) = ForwardDiff.gradient(g, θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimize\n",
    "# res_fourier = # TODO \n",
    "\n",
    "Random.seed!(1234)\n",
    "res_fourier = optimize(g, ∇g, rand(2n), LBFGS(), inplace=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the optimized control\n",
    "a_optimized = [\n",
    "    reshape(expand_in_basis(res_fourier.minimizer[1:end ÷ 2]), (1,T));\n",
    "    reshape(expand_in_basis(res_fourier.minimizer[end ÷ 2:end]), (1,T))\n",
    "]\n",
    "\n",
    "plot_control(a_optimized, \"Fourier series\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
